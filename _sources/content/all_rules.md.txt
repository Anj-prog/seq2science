# Per rule explanation

This is an automatically generated list of all supported rules, their docstrings, and command. At the start of each workflow run a list is printed of which rules will be run. And while the workflow is running it prints which rules are being started and finished. This page is here to give an explanation to the user about what each rule does, and for developers to find what is, and isn't yet supported.

#### bam2cram
Convert a bam file to the more compressed cram format.
```
Generate the index for a cram file.
```

#### bam_bigwig
Convert a bam file into a bigwig file
```
bamCoverage --bam {input.bam} --outFileName {output} --numberOfProcessors {threads} {params} --verbose >> {log} 2>&1
```

#### bam_stranded_bigwig
Convert a bam file into two bigwig files, one for each strand.
TODO: split this rule into two or better: forward & backward logic
```
  
direction1=forward
direction2=reverse
if [ {params.strandedness} == 'reverse' ]; then
direction1=reverse
direction2=forward
fi

bamCoverage --bam {input.bam} --outFileName {output.forwards} --filterRNAstrand $direction1
--numberOfProcessors {threads} {params.flags} --verbose >> {log} 2>&1 &&
bamCoverage --bam {input.bam} --outFileName {output.reverses} --filterRNAstrand $direction2
--numberOfProcessors {threads} {params.flags} --verbose >> {log} 2>&1
```

#### bedgraph_bigwig
Convert a bedgraph file into a bigwig.
```
awk -v OFS='\\t' '{{print $1, $2, $3, $4}}' {input.bedgraph} | sed '/experimental/d' |
bedSort /dev/stdin {output.tmp} > {log} 2>&1;
bedGraphToBigWig {output.tmp} {input.genome_size} {output.out} >> {log} 2>&1
```

#### bedgraphish_to_bedgraph
Convert the bedgraph-ish file generated by genrich into true bedgraph file.
```
splits=$(grep -Pno "([^\/]*)(?=\.bam)" {input})
splits=($splits)
lncnt=$(wc -l {input} | echo "$(grep -Po ".*(?=\ )")":)
splits+=($lncnt)

counter=1
for split in "${{splits[@]::${{#splits[@]}}-1}}";
do
filename=$(grep -Po "(?<=:).*" <<< $split);
if [[ $filename =~ {wildcards.sample} ]]; then
startnr=$(grep -Po ".*(?=\:)" <<< $split);
endnr=$(grep -Po ".*(?=\:)" <<< ${{splits[counter]}});

lines="NR>=startnr && NR<=endnr {{ print \$1, \$2, \$3, \$4 }}"
lines=${{lines/startnr/$((startnr + 2))}}
lines=${{lines/endnr/$((endnr - 1))}}

awk "$lines" {input} > {output}
fi
((counter++))
done
```

#### blind_clustering
Create a sample distance matrix plot per assembly

#### bowtie2_align
Align reads against a genome (index) with bowtie2, and pipe the output to the required sorter(s).

#### bowtie2_index
Make a genome index for bowtie2. This index is required for faster alignment.
```
Align reads against a genome (index) with bowtie2, and pipe the output to the required sorter(s).
```

#### bwa_index
Make a genome index for bwa (mem). This index is required for faster alignment.
```
Align reads against a genome (index) with bwa-mem, and pipe the output to the required sorter(s).
```

#### bwa_mem
Align reads against a genome (index) with bwa-mem, and pipe the output to the required sorter(s).

#### call_peak_genrich
Call peaks with genrich based on the pileup.
```
Call peaks using macs2.
Macs2 requires a genome size, which we estimate from the amount of unique kmers of the average read length.
```

#### complement_blacklist
Take the complement of the blacklist.

#### computeMatrix
Pre-compute correlations between bams using deeptools.
```
computeMatrix scale-regions -S {input.bw} {params.labels} -R {params.annotation} -p {threads} -b 2000 -a 500 -o {output} > {log} 2>&1
```

#### counts_matrix
Merge gene counts per assembly

From https://github.com/snakemake-workflows/rna-seq-star-deseq2

#### create_SNAP_object
Create a snapobject for each BAM file. 

These snapobjects can be merged later using snaptools in R.
```
snaptools snap-pre --input-file={input.bams} --output-snap={output} --genome-name={wildcards.assembly} \
--genome-size={input.genome_size} {params.params} {params.chrm} {params.mapq} > {log} 2>&1
```

#### create_bins_SNAP_object
Add a Binned genome matrix to the SNAPobject, after which it is renamed and moved
to the Snapfiles folder for downstream analysis in R using Snaptools
```
snaptools snap-add-bmat --snap-file={input} {params} > {log} 2>&1
echo "bmat added, moving file" >> {log} 2>&1
mv {input} {output}	>> {log} 2>&1
```

#### cytoband
Generate a cytoband track for each assembly

source: http://genomewiki.ucsc.edu/index.php/Assembly_Hubs#Cytoband_Track
```
        cat {input.sizes} | bedSort /dev/stdin /dev/stdout | awk '{{print $1,0,$2,$1,"gneg"}}' > {output.cytoband_bd}

        bedToBigBed -type=bed4 {output.cytoband_bd} -as={params.schema} {input.sizes} {output.cytoband_bb} >> {log} 2>&1
        
```

#### decoy_transcripts
Generate decoy_transcripts.txt for Salmon indexing  

script source: https://github.com/COMBINE-lab/SalmonTools

#### deseq2
Differential gene expression analysis with DESeq2.

#### fastqc
Generate quality control report for fastq files.
```
Get insert size metrics from a (paired-end) bam. This score is then used by
MultiQC in the report.
```

#### featureCounts
Use featureCounts to generate the fraction reads in peaks score (frips/assigned reads).
https://www.biostars.org/p/337872/
https://www.biostars.org/p/228636/

#### gcPercent
Generate a gc content track

source: http://hgdownload.cse.ucsc.edu/goldenPath/hg19/gc5Base/

#### genrich_pileup
Generate the pileup. We do this separately from peak-calling since these two processes 
have a very different computational footprint.
```
input=$(echo {input.reps} | tr ' ' ',')
Genrich -X -t $input -f {output.log} {params.control} -k {output.bedgraphish} {params.params} -v > {log} 2>&1
```

#### get_genome
Download a genome through genomepy.
Additionally downloads the gene annotation if required downstream.

If assemblies with the same name can be downloaded from multiple providers, 
a provider may be specified in the config (example: provider: NCBI). Otherwise,
each provider will be tried in turn, stopping at the first success.

Automatically turns on/off plugins.
```
# turn off plugins and reset on exit. delete temp files on exit.
active_plugins=$(genomepy config show | grep -Po '(?<=- ).*' | paste -s -d, -) || echo ""
trap "genomepy plugin enable {{$active_plugins,}} >> {log} 2>&1; rm -f {params.temp}" EXIT
genomepy plugin disable {{blacklist,bowtie2,bwa,star,gmap,hisat2,minimap2}} >> {log} 2>&1
genomepy plugin enable {{blacklist,}} >> {log} 2>&1

# download the genome and attempt to download the annotation
if [[ ! {params.provider} = None  ]]; then
genomepy install --genomes_dir {params.dir} {wildcards.assembly} {params.provider} --annotation >> {log} 2>&1
else
genomepy install --genomes_dir {params.dir} {wildcards.assembly} Ensembl --annotation >> {log} 2>&1 ||
genomepy install --genomes_dir {params.dir} {wildcards.assembly} UCSC--annotation >> {log} 2>&1 ||
genomepy install --genomes_dir {params.dir} {wildcards.assembly} NCBI--annotation >> {log} 2>&1
fi

# unzip annotation if downloaded, warn if required but empty
if [ -f {params.gtf} ]; then
gunzip {params.gtf} >> {log} 2>&1
elif echo {output} | grep -q annotation.gtf; then
echo '\nAnnotation for {wildcards.assembly} contains no genes. Select a different assembly or provide an annotation file manually.\n\n' > {log}
exit 1
fi
```

#### get_transcripts
Generate transcripts.fasta using gffread.

Requires genome.fa and annotation.gtf (with matching chromosome/scaffold names)
```
Generate decoy_transcripts.txt for Salmon indexing  

script source: https://github.com/COMBINE-lab/SalmonTools
```

#### hisat2_align
Align reads against a genome (index) with hisat2, and pipe the output to the required sorter(s).

#### hisat2_index
Make a genome index for hisat2. This index is required for faster alignment.
```
Align reads against a genome (index) with hisat2, and pipe the output to the required sorter(s).
```

#### hmmratac
Call gappedpeaks with HMMRATAC.
```
HMMRATAC --bedgraph true -o {params.basename} {params.hmmratac_params} -Xmx22G -b {input.bam} -i {input.bam_index} -g {input.genome_size} > {log} 2>&1
```

#### hmmratac_genome_info
Generate the 'genome info' that hmmratac requires for peak calling.
https://github.com/LiuLabUB/HMMRATAC/issues/17

TODO: isnt this just .fa.sizes?

#### id2sra
Download the SRA of a sample by its unique identifier.

Tries first downloading with the faster ascp protocol, if that fails it 
falls back on the slower http protocol.
```
# setup tmp dir
tmpdir={config['sra_dir']}/tmp/{{wildcards.sample}}
mkdir -p $tmpdir; trap "rm -rf $tmpdir" EXIT

# dump to tmp dir
parallel-fastq-dump -s {{input}}/* -O $tmpdir {config['splot']} --threads {{threads}} --gzip >> {{log}} 2>&1

# rename file and move to output dir
for f in $(ls -1q $tmpdir | grep -oP "^[^_]+" | uniq); do
dst={config['fastq_dir']}/{{wildcards.sample}}.{config['fqsuffix']}.gz
cat "${{{{tmpdir}}}}/${{{{f}}}}_pass.fastq.gz" >> $dst
done
```

#### idr
Combine replicates based on the irreproducible discovery rate (IDR). Can only handle two replicates.
For more than two replicates use fisher's method.
```
if [ "{params.nr_reps}" == "1" ]; then
cp {input} {output}
else
idr --samples {input} {params.rank} --output-file {output} > {log} 2>&1
fi
```

#### insert_size_metrics
Get insert size metrics from a (paired-end) bam. This score is then used by
MultiQC in the report.

#### keep_mates
In-house script that, after alignment, removes the information that reads are paired.
This can be beneficial when peak calling with macs2 when shifting + extending, since
macs2 in this case only keeps the first in pair.
```
samtools view -H {input} 2>  {log} | grep SQ 2>> {log} | cut -f 2-3 2>> {log} | cut -d ':' -f 2   2>> {log} | cut -f 1> {output.tmp1} 2> {log}
samtools view -H {input} 2>> {log} | grep SQ 2>> {log} | cut -f 2-3 2>> {log} | cut -d ':' -f 2,3 2>> {log} | cut -d ':' -f 2 > {output.tmp2} 2> {log}
paste {output.tmp1} {output.tmp2} > {output.out} 2> {log}
```

#### linked_txome
Generate a linked transcriptome for tximeta

Also creates a symlink to the gtf in an Ensembl format (required by tximeta)

Required to converting salmon output (estimated transcript abundances) to gene counts

#### macs2_callpeak
Call peaks using macs2.
Macs2 requires a genome size, which we estimate from the amount of unique kmers of the average read length.

#### macs_bdgcmp
Prepare p-value files for rule macs_cmbreps
```
macs2 bdgcmp -t {input.treatment} -c {input.control} -m qpois -o {output} > {log} 2>&1
```

#### macs_cmbreps
Combine replicates through Fisher's method

(Link original peakfile in replicate_processed if there is only 1 sample for a condition)
```
if [ "{params.nr_reps}" == "1" ]; then
touch {output.tmpbdg} {output.tmppeaks}
mkdir -p $(dirname {output.peaks}); cp {input.treatment} {output.peaks}
else
macs2 cmbreps -i {input.bdgcmp} -o {output.tmpbdg} -m fisher > {log} 2>&1
macs2 {params.function} {params.config} -i {output.tmpbdg} -o {output.tmppeaks} >> {log} 2>&1
cat {output.tmppeaks} | tail -n +2 > {output.peaks}
fi
```

#### mark_duplicates
Mark all duplicate reads in a bam file with picard MarkDuplicates
```
Create an index of a bam file which can be used for e.g. visualization.
```

#### merge_replicates
Merge replicates (fastqs) simply by concatenating the files. We also change the name of the read headers to 
contain the name of the original replicate.

Must happen after trimming due to trim-galore's automatic adapter trimming method 

If a replicate has only 1 sample in it, simply move the file.

#### mt_nuc_ratio_calculator
Estimate the amount of nuclear and mitochondrial reads in a sample. This metric
is especially important in ATAC-seq experiments where mitochondrial DNA can
be overrepresented. Reads are classified as mitochondrial reads if they map
against either "chrM" or "MT".

These values are aggregated and displayed in the MultiQC report.
```
mtnucratio {input.bam} {params.mitochondria}
```

#### multiBamSummary
Pre-compute a bam summary with deeptools.
```
multiBamSummary bins --bamfiles {input.bams} -out {output} {params} -p {threads} > {log} 2>&1
```

#### multiqc
Aggregate all the quality control metrics for every sample into a single multiqc report.

#### multiqc_filter_buttons
Generate filter buttons.

#### multiqc_header_info
Generate a multiqc header file with contact info and date of multiqc generation.
```
multiqc {input.files} -o {params.dir} -n multiqc_{wildcards.assembly}.html \
--config {input.schema}\
--config {input.header}\
--sample-names {input.sample_names}\
--sample-filters {input.filter_buttons}\
--cl_config "extra_fn_clean_exts: [\
{{'pattern': ^.*{wildcards.assembly}-, 'type': 'regex'}},  \
{{'pattern': {params.fqext1},  'type': 'regex'}},  \
]" > {log} 2>&1
```

#### multiqc_rename_buttons
Generate rename buttons for the multiqc report.

#### multiqc_samplesconfig
Add a section in the multiqc report that reports the samples.tsv and config.yaml

#### multiqc_schema


#### peak_bigpeak
Convert a narrowpeak file into a bignarrowpeak file.
https://genome-source.gi.ucsc.edu/gitlist/kent.git/tree/master/src/hg/lib/
```
# keep first 10 columns, idr adds extra columns we do not need for our bigpeak
cut -d$'\t' -f 1-{params.columns} {input.narrowpeak} |
awk -v OFS="\t" '{{$5=$5>1000?1000:$5}} {{print}}' | 
bedSort /dev/stdin {output.tmp} > {log} 2>&1;
bedToBigBed -type={params.type} -as={params.schema} {output.tmp} {input.genome_size} {output.out} > {log} 2>&1
```

#### peak_union
TODO improve + explain
```
TODO improve + explain
```

#### plotCorrelation
Calculate the correlation between bams with deeptools.
```
plotCorrelation --corData {input} --outFileCorMatrix {output} -c spearman -p heatmap > {log} 2>&1
```

#### plotFingerprint
Plot the "fingerprint" of your bams, using deeptools. 
```
plotFingerprint -b {input.bams} {params} --outRawCounts {output} -p {threads} > {log} 2>&1 
```

#### plotPCA
Plot a PCA between bams using deeptools.
```
plotPCA --corData {input} --outFileNameData {output} > {log} 2>&1
```

#### plotProfile
Plot the so-called profile using deeptools.
```
plotProfile -m {input} --outFileName {output.img} --outFileNameData {output.file} > {log} 2>&1
```

#### renamefastq_PE
Create symlinks to fastqs with incorrect fqexts (default R1/R2).
Forward and reverse samples will be switched if forward/reverse names are not 
lexicographically ordered.
```
ln {input[0]} {output[0]}
ln {input[1]} {output[1]}
```

#### salmon_decoy_aware_index
Make a decoy aware transcript index for Salmon.
```
salmon index -t {input.transcripts} --decoys {input.decoy_transcripts} -i {output} {params} \
--threads {threads} > {log} 2>&1
```

#### salmon_index
Make a transcriptomic index for Salmon.
```
salmon index -t {input} -i {output} {params} --threads {threads} > {log} 2>&1
```

#### salmon_quant
Align reads against a transcriptome (index) with Salmon (mapping-based mode) and output a quantification file per sample.
```
salmon quant -i {input.index} -l A {params.input} {params.params} -o {output.dir} \
--threads {threads} > {log} 2>&1
```

#### sambamba_sort
Sort the result of alignment or sieving with the sambamba sorter.

#### samtools_index
Create an index of a bam file which can be used for e.g. visualization.

#### samtools_index_cram
Generate the index for a cram file.

#### samtools_presort
(Pre)sort the result of alignment with the samtools sorter.
```
trap "rm -f {params.out_dir}/{wildcards.assembly}-{wildcards.sample}.tmp*bam" INT;
samtools sort {params.sort_order} -@ {params.threads} {input} -o {output} -T {params.out_dir}/{wildcards.assembly}-{wildcards.sample}.tmp 2> {log}
```

#### samtools_sort
Sort the result of sieving with the samtools sorter.
```
trap "rm -f {params.out_dir}/{wildcards.assembly}-{wildcards.sample}.tmp*bam" INT;
samtools sort {params.sort_order} -@ {params.threads} {input} -o {output} -T {params.out_dir}/{wildcards.assembly}-{wildcards.sample}.tmp 2> {log}
```

#### samtools_stats
Get general stats from bam files like percentage mapped.
```
Use featureCounts to generate the fraction reads in peaks score (frips/assigned reads).
https://www.biostars.org/p/337872/
https://www.biostars.org/p/228636/
```

#### setup_blacklist
Combine the encode blacklist with mitochrondrial dna depending on config.
```
sortBed -faidx {input.sizes} -i {input.blacklist} |
complementBed -i stdin -g {input.sizes} > {output} 2> {log}
```

#### sieve_bam
"Sieve" a bam.

This rule does (at least) one of these:
* filtering on minimum mapping quality
* tn5 shift adjustment
* remove multimappers
* remove reads inside the blacklist
```
Sort the result of alignment or sieving with the sambamba sorter.
```

#### softmask_track_1
Generate a track of all softmasked regions

source: https://github.com/Gaius-Augustus/MakeHub/blob/master/make_hub.py
```
bedSort {input.mask_unsorted} {output.maskbed} >> {log} 2>&1

bedToBigBed -type=bed3 {output.maskbed} {input.sizes} {output.mask} >> {log} 2>&1
```

#### softmask_track_2
Generate a track of all softmasked regions

source: https://github.com/Gaius-Augustus/MakeHub/blob/master/make_hub.py

#### sra2fastq_PE
Downloaded (raw) SRAs are converted to paired-end fastq files.
Forward and reverse samples will be switched if forward/reverse names are not lexicographically ordered.
```
# setup tmp dir
tmpdir={config['sra_dir']}/tmp/{{wildcards.sample}}
mkdir -p $tmpdir; trap "rm -rf $tmpdir" EXIT

# dump to tmp dir
parallel-fastq-dump -s {{input}}/* -O $tmpdir {config['split']} --threads {{threads}} --gzip >> {{log}} 2>&1

# rename files and move to output dir
for f in $(ls -1q $tmpdir | grep -oP "^[^_]+" | uniq); do
dst_1={config['fastq_dir']}/{{wildcards.sample}}_{config['fqext1']}.{config['fqsuffix']}.gz
dst_2={config['fastq_dir']}/{{wildcards.sample}}_{config['fqext2']}.{config['fqsuffix']}.gz
cat "${{{{tmpdir}}}}/${{{{f}}}}_pass_1.fastq.gz" >> $dst_1
cat "${{{{tmpdir}}}}/${{{{f}}}}_pass_2.fastq.gz" >> $dst_2
done
```

#### sra2fastq_SE
Downloaded (raw) SRAs are converted to single-end fastq files.

#### star_align
Align reads against a genome (index) with STAR, and pipe the output to the required sorter(s).
```
trap "find {log} -type f ! -name Log* -exec rm {{}} \;" EXIT
mkdir -p {log}
mkdir -p {output.dir}    

STAR --genomeDir {input.index} --readFilesIn {params.input} --readFilesCommand gunzip -c \
--quantMode GeneCounts --outSAMtype BAM Unsorted --outStd BAM_Unsorted \
--outFileNamePrefix {log}/ --outTmpDir {output.dir}/STARtmp \
--runThreadN {threads} {params.params} > {output.pipe} 2> {log}/Log.stderr.out

# move all non-log files to output directory (this way the log files are kept on error)
find {log} -type f ! -name Log* -exec mv {{}} {output.dir} \;
```

#### star_index
Make a genome index for STAR.

Troubleshooting:
1) sufficient disk space?
2) increase the RAM available (--limitGenomeGenerateRAM)
3) reduce the number of threads (snakemake -j 5)
4) reduce accuracy (--genomeSAsparseD 2)

For example, in your config.yaml, set aligner/quantifier:
aligner:
star:
index: --limitGenomeGenerateRAM 60000000000 --genomeSAsparseD 1
```
            function log2 {{
local x=0
for (( y=$1-1 ; $y > 0; y >>= 1 )) ; do
    let x=$x+1
done
echo $x
            }}
            
            # set genome dependent variables
            NBits=""
            NBases=""
            GenomeLength=$(awk -F"\t" '{{x+=$2}}END{{printf "%i", x}}' {input.sizefile})
            NumberOfReferences=$(awk 'END{{print NR}}' {input.sizefile})
            if [ $NumberOfReferences -gt 5000 ]; then
# for large genomes, --genomeChrBinNbits should be scaled to min(18,log2[max(GenomeLength/NumberOfReferences,ReadLength)])
# ReadLength is skipped here, as it is unknown
LpR=$(log2 $((GenomeLength / NumberOfReferences)))
NBits="--genomeChrBinNbits $(($LpR<18 ? $LpR : 18))"
printf "NBits: $NBits\n\n" >> {log} 2>&1
            fi
            
            if [ $GenomeLength -lt 268435456 ]; then
# for small genomes, --genomeSAindexNbases must be scaled down to min(14, log2(GenomeLength)/2-1)
logG=$(( $(log2 $GenomeLength) / 2 - 1 ))
NBases="--genomeSAindexNbases $(( $logG<14 ? $logG : 14 ))"
printf "NBases: $NBases\n\n" >> {log} 2>&1
            fi
            
            mkdir -p {output}
            
            STAR --runMode genomeGenerate --genomeFastaFiles {input.genome} --sjdbGTFfile {input.gtf} \
            --genomeDir {output} --outFileNamePrefix {output}/ \
            --runThreadN {threads} $NBits $NBases {params} >> {log} 2>&1
            
```

#### star_quant
Quantify reads against a genome and transcriptome (index) with STAR and output a counts table per sample.
```
trap "find {log} -type f ! -name Log* -exec rm {{}} \;" EXIT
mkdir -p {log}
mkdir -p {output.dir}    

STAR --genomeDir {input.index} --readFilesIn {params.input} --readFilesCommand gunzip -c \
--quantMode GeneCounts --outSAMtype None \
--outFileNamePrefix {log}/ --outTmpDir {output.dir}/STARtmp \
--runThreadN {threads} {params.params} > {log}/Log.std_stderr.out 2>&1

# move all non-log files to output directory (this way the log files are kept on error)
find {log} -type f ! -name Log* -exec mv {{}} {output.dir} \;
```

#### trackhub
Generate a trackhub which has to be hosted on an web accessible location, 
but can then be viewed through the UCSC genome browser.

#### trackhub_index
Generate a searchable annotation & index for each assembly

source: https://genome.ucsc.edu/goldenPath/help/hubQuickStartSearch.html
```
# generate annotation files
gtfToGenePred -geneNameAsName2 -genePredExt {input.gtf} {output.genePred} -infoOut={output.info} >> {log} 2>&1

genePredToBed {output.genePred} {output.genePredbed} >> {log} 2>&1

bedSort {output.genePredbed} {output.genePredbed} >> {log} 2>&1

bedToBigBed -extraIndex=name {output.genePredbed} {input.sizes} {output.genePredbigbed} >> {log} 2>&1

# generate searchable indexes (by 2: geneId, 8: proteinID, 9: geneName, 10: transcriptName and 1: transcriptID)
grep -v "^#" {output.info} | awk '{{print $1, $2, $8, $9, $10, $1}}' > {output.indexinfo}

ixIxx {output.indexinfo} {output.ix} {output.ixx} >> {log} 2>&1
```

#### trim_galore_PE
Automated adapter detection, adapter trimming, and quality trimming through trim galore (paired-end).
```
trim_galore --paired -j {threads} {params.config} -o $(dirname {output.r1}) {input.r1} {input.r2} > {log} 2>&1

# now rename to proper output
for f in $(find "$(dirname {output.r1})/" -name "{wildcards.sample}_*val_*.fq.gz"); do
mv "$f" "$(echo "$f" | sed s/.fq/.{params.fqsuffix}/)"; done
for f in $(find "$(dirname {output.r1})/" -maxdepth 1 -name "{wildcards.sample}_*val_*.{params.fqsuffix}.gz"); do
mv "$f" "$(echo "$f" | sed s/_val_./_trimmed/)"; done

# move the trimming reports to qc directory
for f in $(find "$(dirname {output.r1})/" -name "{wildcards.sample}_*.{params.fqsuffix}.gz_trimming_report.txt"); do
mv "$f" "$(dirname {output.qc[0]})/$(basename $f)"; done
```

#### trim_galore_SE
Automated adapter detection, adapter trimming, and quality trimming through trim galore (single-end).
```
trim_galore -j {threads} {params.config} -o $(dirname {output.se}) {input} > {log} 2>&1

# now rename to proper output
if [ "{params.fqsuffix}" != "fq" ]; then
  mv "$(dirname {output.se})/{wildcards.sample}_trimmed.fq.gz" {output.se}
fi 

# move the trimming report to qc directory
report=$(dirname {output.se})/{wildcards.sample}.{params.fqsuffix}.gz_trimming_report.txt
mv $report {output.qc}
```

#### twobit
Generate a 2bit file for each assembly
```
Generate a gc content track

source: http://hgdownload.cse.ucsc.edu/goldenPath/hg19/gc5Base/
```

#### txi_count_matrix
Convert estimated transcript abundances to gene count estimations and merge gene counts per assembly

Also outputs a single cell experiment object similar to ARMOR (https://github.com/csoneson/ARMOR)

Only works with Ensembl assemblies

